{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd0e46-5a29-4405-b15c-750fcd467baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, confusion_matrix, \n",
    "                             classification_report)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c60dd16-5c4a-4db2-8634-28c95d92bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904fbe77-90f5-463f-9bdd-5908cadaf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(X_train, y_train, X_test, feature_names, k=30):\n",
    "    \"\"\"\n",
    "    Select top k features using ANOVA F-value\n",
    "    \n",
    "    Parameters:\n",
    "    X_train: numpy array of scaled training features\n",
    "    y_train: numpy array of training target\n",
    "    X_test: numpy array of scaled test features\n",
    "    feature_names: list of original feature names\n",
    "    k: number of top features to select\n",
    "    \"\"\"\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Get selected feature names using the original feature names\n",
    "    selected_features = [feature_names[i] for i in selector.get_support(indices=True)]\n",
    "    \n",
    "    return X_train_selected, X_test_selected, selected_features\n",
    "\n",
    "# Get feature names before scaling (when X_train is still a DataFrame)\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "# Select top 30 features (adjust based on your needs)\n",
    "X_train_selected, X_test_selected, selected_features = select_features(\n",
    "    X_train_scaled, y_train_res, X_test_scaled, feature_names, k=30\n",
    ")\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50153675-02e3-4cd6-bdd3-f53d55869709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train_res:\", y_train_res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31220803-30e7-489c-b5e9-5bcbd4898a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde91edb-63eb-4b77-8815-b5ef0d3430c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052a851-da6b-4a8a-a6a1-8e80384ed0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debd284-b3c6-40ec-a517-378aa4535a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                            f1_score, roc_auc_score, confusion_matrix,\n",
    "                            classification_report)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# **After installing the compatible version, run this again to verify:**\n",
    "import sklearn\n",
    "print(f\"Scikit-learn version after potential install: {sklearn.__version__}\")\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_excel('US_Recession.xlsx')  # Update with your actual file path\n",
    "\n",
    "if 'Date' in data.columns:\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df):\n",
    "    df_processed = df.copy()\n",
    "    df_processed.fillna(method='ffill', inplace=True)\n",
    "    df_processed.fillna(method='bfill', inplace=True)\n",
    "    if 'gdp_growth' in df.columns:\n",
    "        for lag in [1, 2, 4]:\n",
    "            df_processed[f'gdp_growth_lag_{lag}'] = df_processed['gdp_growth'].shift(lag)\n",
    "    if 'unemployment_rate' in df.columns:\n",
    "        df_processed['unemployment_change'] = df_processed['unemployment_rate'].diff()\n",
    "    if 'industrial_production' in df.columns:\n",
    "        df_processed['industrial_prod_rolling_mean_4q'] = (\n",
    "            df_processed['industrial_production'].rolling(window=4).mean()\n",
    "        )\n",
    "    df_processed.dropna(inplace=True)\n",
    "    return df_processed\n",
    "\n",
    "processed_data = preprocess_data(data)\n",
    "X = processed_data.drop('recession_indicator', axis=1)\n",
    "y = processed_data['recession_indicator']\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_res = scaler.fit_transform(X_train_res) # Scale the resampled training data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "def select_features(X_train, y_train, X_test, feature_names, k=30):\n",
    "    selector = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    selected_features_indices = selector.get_support(indices=True)\n",
    "    selected_features = [feature_names[i] for i in selected_features_indices]\n",
    "    return X_train_selected, X_test_selected, selected_features, selected_features_indices\n",
    "\n",
    "feature_names = X_train.columns.tolist() # Original feature names\n",
    "\n",
    "# Select features based on the scaled resampled training data and resampled target\n",
    "X_train_selected_res, X_test_selected, selected_features, selected_features_indices = select_features(\n",
    "    X_train_scaled_res, y_train_res, X_test_scaled, feature_names, k=30\n",
    ")\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(n_estimators=150, learning_rate=0.1, max_depth=5, random_state=42),\n",
    "        'XGBoost': XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.05, objective='binary:logistic', random_state=42, scale_pos_weight=sum(y_train==0)/sum(y_train==1))\n",
    "    }\n",
    "    best_model = None\n",
    "    best_score = 0.0\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train) # Use X_train_selected_res here\n",
    "        y_pred = model.predict(X_test) # Use X_test_selected here\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] # Use X_test_selected here\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "        results[name] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'roc_auc': roc_auc}\n",
    "        print(f\"\\n{name} Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title(f'{name} Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.show()\n",
    "        if roc_auc > best_score:\n",
    "            best_score = roc_auc\n",
    "            best_model = model\n",
    "    print(\"\\nModel Comparison:\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "    return best_model\n",
    "\n",
    "# Train and evaluate models using the selected resampled data\n",
    "best_model = train_and_evaluate_models(X_train_selected_res, y_train_res, X_test_selected, y_test)\n",
    "\n",
    "# Create the full pipeline with the *fitted* scaler and feature selector\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer(\n",
    "        [('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', scaler)]),\n",
    "          selected_features)],\n",
    "        remainder='passthrough'\n",
    "    )),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "# **FIT THE PIPELINE TO THE SCALED AND SELECTED RESAMPLED TRAINING DATA:**\n",
    "full_pipeline.fit(X_train_res.iloc[:, selected_features_indices], y_train_res)\n",
    "\n",
    "# Save the *entire* fitted pipeline\n",
    "joblib.dump(full_pipeline, 'vertex_model.joblib')\n",
    "print(\"✅ Full pipeline saved as vertex_model.joblib\")\n",
    "\n",
    "# Upload the pipeline to GCS\n",
    "from google.cloud import storage\n",
    "client = storage.Client.from_service_account_json(\n",
    "    r\"C:/Users/rudra.s/Downloads/spry-scope-456205-m7-be989ff0ac36.json\"\n",
    ")\n",
    "blob = client.bucket('us-recession-model').blob('vertex_model.joblib')\n",
    "blob.upload_from_filename('vertex_model.joblib', content_type='application/joblib')\n",
    "print(\"✅ Pipeline uploaded to gs://us-recession-model/vertex_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15b285-9ee1-4caa-8f33-ece72ca3ddde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7c908-d9c2-4110-91f9-da51c034831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a0b0e-eb96-4224-a2df-2030ec36efc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
